{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gzip, os, csv\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[10] appName=pyspark-shell>\n"
     ]
    }
   ],
   "source": [
    "if False: \n",
    "    sc.stop()\n",
    "\n",
    "config = SparkConf()\n",
    "config.setMaster(\"local[10]\")\n",
    "config.set(\"spark.executor.memory\", \"10g\")\n",
    "config.set('spark.driver.memory', '10g')\n",
    "config.set(\"spark.memory.offHeap.enabled\",True)\n",
    "config.set(\"spark.memory.offHeap.size\",\"10g\") \n",
    "sc = SparkContext(conf=config)\n",
    "print (sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addTriple(net, source, target, edge):\n",
    "    if source in net:\n",
    "        if  target in net[source]:\n",
    "            net[source][target].add(edge)\n",
    "        else:\n",
    "            net[source][target]= set([edge])\n",
    "    else:\n",
    "        net[source]={}\n",
    "        net[source][target] =set([edge])\n",
    "            \n",
    "def getLinks(net, source):\n",
    "    if source not in net:\n",
    "        return {}\n",
    "    return net[source]\n",
    "\n",
    "def randomWalkUniform(triples, startNode, max_depth=5):\n",
    "    next_node =startNode\n",
    "    path = 'n'+str(startNode)+'->'\n",
    "    for i in range(max_depth):\n",
    "        neighs = getLinks(triples,next_node)\n",
    "        #print (neighs)\n",
    "        if len(neighs) == 0: break\n",
    "        weights = []\n",
    "        queue = []\n",
    "        for neigh in neighs:\n",
    "            for edge in neighs[neigh]:\n",
    "                queue.append((edge,neigh))\n",
    "                weights.append(1.0)\n",
    "        edge, next_node = random.choice(queue)\n",
    "        path = path+ 'e'+str(edge)+'->'\n",
    "        path = path+ 'n'+str(next_node)+'->'\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(folders, filename):\n",
    "    entity2id = {}\n",
    "    relation2id = {}\n",
    "    triples = {}\n",
    "\n",
    "    ent_counter = 0\n",
    "    rel_counter = 0\n",
    "    for dirname in folders:\n",
    "        for fname in os.listdir(dirname):\n",
    "            if not filename in fname: continue\n",
    "            print (fname)\n",
    "            if fname.endswith('.gz'):\n",
    "                kgfile = gzip.open(os.path.join(dirname, fname), mode='rt')\n",
    "            else:\n",
    "                kgfile = open(os.path.join(dirname, fname), mode='rt')\n",
    "                \n",
    "            for line in csv.reader(kgfile, delimiter='\\t', quotechar='\"'):\n",
    "                h = line[0]\n",
    "                r = line[1]\n",
    "                t = line[2]\n",
    "\n",
    "                if h in entity2id:\n",
    "                    hid = entity2id[h]\n",
    "                else:\n",
    "                    entity2id[h] = ent_counter\n",
    "                    ent_counter+=1\n",
    "                    hid = entity2id[h]\n",
    "\n",
    "                if t in entity2id:\n",
    "                    tid = entity2id[t]\n",
    "                else:\n",
    "                    entity2id[t] = ent_counter\n",
    "                    ent_counter+=1\n",
    "                    tid = entity2id[t]\n",
    "\n",
    "                if r in relation2id:\n",
    "                    rid = relation2id[r]\n",
    "                else:\n",
    "                    relation2id[r] = rel_counter\n",
    "                    rel_counter+=1\n",
    "                    rid = relation2id[r]\n",
    "                addTriple(triples, hid, tid, rid)\n",
    "            print ('Relation:',rel_counter, ' Entity:',ent_counter)\n",
    "    return entity2id,relation2id,triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drug-disease-kg.nt\n",
      "Relation: 8  Entity: 22678\n"
     ]
    }
   ],
   "source": [
    "folders = ['data/graph/']\n",
    "fileext = '.nt'\n",
    "entity2id, relation2id, triples = preprocess(folders, fileext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of triples 974840\n"
     ]
    }
   ],
   "source": [
    "num_triples=0\n",
    "for source in triples:\n",
    "    for  target in triples[source]:\n",
    "        num_triples+=len(triples[source][target])\n",
    "print ('Number of triples',num_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomNWalkUniform(triples, n, walks, path_depth):\n",
    "    path=[]\n",
    "    for k in range(walks):\n",
    "        walk = randomWalkUniform(triples, n, path_depth)\n",
    "        path.append(walk)\n",
    "    path = list(set(path))\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n100->e0->n2861->e1->n2917->e0->n7156->e1->n476->e0->n7143->e1->n1268->e0->n7307->e1->n2030->e0->n7206->e1->n6706->\n",
      "n100->e0->n7215->e1->n6195->e0->n7212->e1->n290->e0->n7190->e1->n972->e0->n6749->e1->n754->e0->n7305->e1->n5447->\n",
      "n100->e0->n7214->e1->n85->e0->n7295->e1->n5017->e0->n7267->e1->n5078->e4->n9669->e5->n1745->e4->n12752->e5->n1678->\n",
      "n100->e0->n6839->e1->n6219->e0->n7204->e1->n1322->e0->n7197->e1->n6331->e0->n7200->e1->n3962->e0->n7302->e1->n2217->\n",
      "n100->e0->n7291->e1->n2707->e0->n6979->e1->n4471->e0->n7304->e1->n161->e0->n7287->e1->n6969->e4->n9248->e5->n5411->\n"
     ]
    }
   ],
   "source": [
    "walks = 5\n",
    "path_depth = 10\n",
    "paths = randomNWalkUniform(triples, 100, walks, path_depth)\n",
    "print('\\n'.join(paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entities = list(entity2id.values())\n",
    "b_triples = sc.broadcast(triples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./walks/randwalks_n250_depth1_pagerank_uniform.txt\n",
      "Time elapsed to generate features: 00:00:35\n",
      "./walks/randwalks_n250_depth2_pagerank_uniform.txt\n",
      "Time elapsed to generate features: 00:09:03\n",
      "./walks/randwalks_n250_depth3_pagerank_uniform.txt\n",
      "Time elapsed to generate features: 00:10:42\n",
      "./walks/randwalks_n250_depth4_pagerank_uniform.txt\n",
      "Time elapsed to generate features: 00:17:57\n"
     ]
    }
   ],
   "source": [
    "folder = './walks/'\n",
    "if not os.path.isdir(folder):\n",
    "    os.mkdir(folder)\n",
    "walks = 250\n",
    "maxDepth = 5\n",
    "for path_depth in range(1,maxDepth):\n",
    "    filename = folder+'randwalks_n%d_depth%d_pagerank_uniform.txt'%(walks, path_depth)\n",
    "    print (filename)\n",
    "    start_time =time.time()\n",
    "    rdd = sc.parallelize(entities).flatMap(lambda n: randomNWalkUniform(b_triples.value, n, walks, path_depth))\n",
    "    rdd.saveAsTextFile(filename)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print ('Time elapsed to generate features:',time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saveData(entity2id, relation2id, triples, dirname):\n",
    "    if not os.path.isdir(dirname):\n",
    "        os.mkdir(dirname)  \n",
    "    \n",
    "    entity2id_file= open(os.path.join(dirname, 'entity2id.txt'),'w')\n",
    "    relation2id_file = open(os.path.join(dirname, 'relation2id.txt'),'w')\n",
    "    train_file = open(os.path.join(dirname, 'train2id.txt'),'w')\n",
    "\n",
    "    train_file.write(str(num_triples)+'\\n') \n",
    "    for source in triples:\n",
    "        for  target in triples[source]:  \n",
    "            hid=source\n",
    "            tid =target\n",
    "            for rid  in triples[source][target]:\n",
    "                train_file.write(\"%d %d %d\\n\"%(hid,tid,rid))\n",
    "\n",
    "    entity2id_file.write(str(len(entity2id))+'\\n')  \n",
    "    for e in sorted(entity2id, key=entity2id.__getitem__):\n",
    "        entity2id_file.write(e+'\\t'+str(entity2id[e])+'\\n')  \n",
    "\n",
    "    relation2id_file.write(str(len(relation2id))+'\\n')    \n",
    "    for r in sorted(relation2id, key=relation2id.__getitem__):\n",
    "        relation2id_file.write(r+'\\t'+str(relation2id[r])+'\\n') \n",
    "        \n",
    "    train_file.close()\n",
    "    entity2id_file.close()\n",
    "    relation2id_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = 'kg_graph'\n",
    "saveData(entity2id, relation2id, triples, dirname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname, filename):\n",
    "        self.dirname = dirname\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        print ('Processing ',self.filename)\n",
    "        for subfname in os.listdir(self.dirname):\n",
    "            if not self.filename in subfname: continue\n",
    "            fpath = os.path.join(self.dirname, subfname)\n",
    "            for fname in os.listdir(fpath):\n",
    "                if not 'part' in fname: continue\n",
    "                if '.crc' in fname: continue\n",
    "                try:\n",
    "                    for line in open(os.path.join(fpath, fname), mode='r'):\n",
    "                        line = line.rstrip('\\n')\n",
    "                        words = line.split(\"->\")\n",
    "                        yield words\n",
    "                except Exception:\n",
    "                    print(\"Failed reading file:\")\n",
    "                    print(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractFeatureVector(model, drugs, id2entity, output): \n",
    "  \n",
    "    header=\"Entity\"\n",
    "    ns = \"n\"\n",
    "    first = ns+str(drugs[0])\n",
    "\n",
    "    for i in range(len(model.wv[first])):\n",
    "        header=header+\"\\tfeature\"+str(i)\n",
    "        \n",
    "    fw=open(output,'w')\n",
    "    fw.write(header+\"\\n\")\n",
    "\n",
    "    for id_ in sorted(drugs):\n",
    "        nid =ns+str(id_)\n",
    "        if  (nid) not in  model.wv:\n",
    "            print (nid)\n",
    "            continue\n",
    "        vec = model.wv[nid]\n",
    "        vec = \"\\t\".join(map(str,vec))\n",
    "        fw.write( id2entity[id_]+'\\t'+str(vec)+'\\n')\n",
    "    fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def trainModel(drugs, id2entity, datafilename, model_output, vector_output, pattern, maxDepth):\n",
    "    \n",
    "    if not os.path.isdir(model_output):\n",
    "        os.mkdir(model_output)\n",
    "        \n",
    "    if not os.path.isdir(vector_output):\n",
    "        os.mkdir(vector_output)\n",
    "    \n",
    "    output = model_output + pattern +'/'\n",
    "    if not os.path.isdir(output):\n",
    "        os.mkdir(output)\n",
    "    \n",
    "    sentences = MySentences(datafilename, filename=pattern) # a memory-friendly iterator\n",
    "    model = gensim.models.Word2Vec(size=200, workers=5, window=5, sg=1, negative=15, iter=5)\n",
    "\n",
    "    model.build_vocab(sentences)\n",
    "    corpus_count = model.corpus_count\n",
    "    #sg/cbow features iterations window negative hops random walks\n",
    "    del model\n",
    "    model1 = gensim.models.Word2Vec(size=200, workers=8, window=5, sg=1, negative=15, iter=5)\n",
    "    model1.build_vocab(sentences)\n",
    "\n",
    "    model1.train(sentences, total_examples=corpus_count, epochs =5)\n",
    "    modelname = 'Entity2Vec_sg_200_5_5_15_2_500'+'_d'+str(maxDepth)\n",
    "    model1.save(output+modelname)\n",
    "    \n",
    "    extractFeatureVector(model1, drugs, id2entity, vector_output+modelname+'_'+pattern+'.txt')\n",
    "    \n",
    "    #cbow 200\n",
    "    del model1\n",
    "    model2 = gensim.models.Word2Vec(size=200, workers=8, window=5, sg=0, iter=5,cbow_mean=1, alpha = 0.05)\n",
    "    model2.build_vocab(sentences)\n",
    "\n",
    "    model2.train(sentences, total_examples=corpus_count, epochs =5)\n",
    "    modelname = 'Entity2Vec_cbow_200_5_5_2_500'+'_d'+str(maxDepth)\n",
    "    model2.save(output+ modelname)\n",
    "    extractFeatureVector(model2, drugs, id2entity, vector_output+modelname+'_'+pattern+'.txt')\n",
    "    del model2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2entity = { value:key for key,value in entity2id.items()} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22678\n"
     ]
    }
   ],
   "source": [
    "kg_entities =list(id2entity.keys())\n",
    "print (len(kg_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing  uniform\n",
      "Processing  uniform\n",
      "Processing  uniform\n",
      "Processing  uniform\n",
      "Processing  uniform\n",
      "Processing  uniform\n",
      "Processing  uniform\n",
      "Processing  uniform\n",
      "Processing  uniform\n",
      "Processing  uniform\n",
      "Processing  uniform\n",
      "Processing  uniform\n",
      "Processing  uniform\n"
     ]
    }
   ],
   "source": [
    "datafilename = './walks/'\n",
    "model_output = './models/'    \n",
    "pattern = 'uniform'\n",
    "vector_output =  './vectors/'\n",
    "trainModel(kg_entities, id2entity, datafilename, model_output, vector_output, pattern, maxDepth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
